import os
import sys
import argparse
import re
import requests  # HTTP library for API calls
import pandas as pd  # For optional ETL work with CSVs
from flask import Flask, request, jsonify
import google.generativeai as genai

# --- Configuration ---
# Load sensitive info like API keys from environment variables
# Example: API_KEY = os.environ.get('MY_API_KEY')
# if not API_KEY:
#     print("Error: API Key environment variable not set.")
#     # sys.exit(1) # Exit if key is essential

LOCAL_DATA_PATH = 'processed_local_data.csv'  # Path to the CSV file generated by ETL
# Configure Gemini API
try:
    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
    if not GOOGLE_API_KEY:
        raise ValueError("GOOGLE_API_KEY environment variable not set.")
    genai.configure(api_key=GOOGLE_API_KEY)
    print("Gemini API Configured.")
except ValueError as e:
    print(f"API Key Configuration Error: {e}", file=sys.stderr)
    # Decide if you want the app to exit or continue with Gemini disabled
    # sys.exit(1) # Or set a flag: gemini_enabled = False
except Exception as e:
    print(f"An unexpected error occurred during Gemini configuration: {e}", file=sys.stderr)
    # sys.exit(1) # Or set a flag: gemini_enabled = False

# -- Spotify credentials (set these in shell / deployment secrets) -
SPOTIFY_CLIENT_ID = os.environ.get("SPOTIFY_CLIENT_ID")
SPOTIFY_CLIENT_SECRET = os.environ.get("SPOTIFY_CLIENT_SECRET")

AUTH_URL = "https://accounts.spotify.com/api/token"
TOP_TRACKS_URL = "https://api.spotify.com/v1/artists/{artist_id}/top-tracks"


# Spotify Helper Functions

def get_spotify_access_token() -> str:
    """Authenticate with Spotify via Client‑Credentials flow and get a bearer token."""
    if not SPOTIFY_CLIENT_ID or not SPOTIFY_CLIENT_SECRET:
        raise RuntimeError(
            "Spotify credentials not set. Please export SPOTIFY_CLIENT_ID and SPOTIFY_CLIENT_SECRET."
        )

    resp = requests.post(
        AUTH_URL,
        data={"grant_type": "client_credentials"},
        auth=(SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET),
        timeout=10,
    )
    resp.raise_for_status()
    return resp.json()["access_token"]


def fetch_artist_top_tracks(artist_id: str, market: str = "US") -> str:
    """Return a formatted string with an artist's top tracks for the given market."""
    token = get_spotify_access_token()
    headers = {"Authorization": f"Bearer {token}"}
    params = {"market": market}

    resp = requests.get(
        TOP_TRACKS_URL.format(artist_id=artist_id),
        headers=headers,
        params=params,
        timeout=10,
    )
    resp.raise_for_status()
    tracks = resp.json().get("tracks", [])

    if not tracks:
        return f"No top tracks found for artist ID {artist_id}."

    lines = [f"Top tracks for {artist_id} (market={market}):"]
    for idx, t in enumerate(tracks, start=1):
        name = t["name"]
        album = t["album"]["name"]
        preview = t.get("preview_url") or "—"
        lines.append(f"{idx:2}. {name}  –  {album}  |  Preview: {preview}")
    return "\n".join(lines)


# --- ETL Section ---
def extract_raw_data():
    """Fetches raw data from the external source."""
    print("ETL: Extracting raw data...")
    # Replace with your actual data extraction logic
    raw_data = None
    print("ETL: Extraction complete.")
    return raw_data

def transform_data(raw_data):
    """Transforms the raw data."""
    print("ETL: Transforming data...")
    if raw_data is None:
        print("ETL: No raw data to transform.")
        return None
    # Replace with your actual data transformation logic (using pandas or other tools)
    transformed_data = raw_data # Placeholder
    print("ETL: Transformation complete.")
    return transformed_data

def load_processed_data(transformed_data, output_path):
    """Loads the transformed data into the local CSV file."""
    print(f"ETL: Loading data to {output_path}...")
    if transformed_data is None:
        print("ETL: No transformed data to load.")
        return
    try:
        # Example using pandas:
        # transformed_data.to_csv(output_path, index=False)
        # Replace with your actual data loading logic
        print(f"ETL: Data loaded successfully to {output_path}.")
    except Exception as e:
        print(f"ETL: Error loading data: {e}")

def run_etl_pipeline():
    """Runs the full ETL pipeline."""
    raw = extract_raw_data()
    transformed = transform_data(raw)
    load_processed_data(transformed, LOCAL_DATA_PATH)


# --- Data Handling Section ---
def query_local_data(user_query):
    """Queries the processed local CSV data."""
    print(f"Querying local data for: {user_query}")
    try:
        if not os.path.exists(LOCAL_DATA_PATH):
            return f"Local data file ({LOCAL_DATA_PATH}) not found. Please run the ETL first."

        # Example using pandas:
        # df = pd.read_csv(LOCAL_DATA_PATH)
        # result = df[df['some_column'].str.contains(user_query, case=False)] # Example query
        # if not result.empty:
        #     return f"Found in local data: {result.to_string()}" # Placeholder response
        # else:
        #     return "Query not found in local data."

        # Replace with your actual CSV querying logic
        return "Placeholder response from local data." # Placeholder
    except Exception as e:
        print(f"Error querying local data: {e}")
        return "Error accessing local data."

def query_api(user_query):
    """Queries the external API."""
    print(f"Querying API for: {user_query}")
    try:
        # params = {'query': user_query, 'apiKey': API_KEY} # Example params
        # response = requests.get(API_ENDPOINT, params=params)
        # response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
        # api_data = response.json()
        # Process api_data to get the answer
        # return f"Found in API: {api_data}" # Placeholder response

        # Replace with your actual API call and processing logic
         return "Placeholder response from API." # Placeholder
    except requests.exceptions.RequestException as e:
        print(f"Error querying API: {e}")
        return "Error contacting the external API."
    except Exception as e:
        print(f"Error processing API response: {e}")
        return "Error processing API data."


# --- Chatbot Logic Section ---
# Initialize the Gemini model (choose one, e.g., flash for speed/cost)
# Do this once outside the function if possible, or handle potential re-init if needed.
try:
    # For safety settings, see: https://ai.google.dev/gemini-api/docs/safety-settings
    generation_config = genai.GenerationConfig(
        temperature=0.7 # Adjust creativity (0.0 = deterministic, 1.0 = creative)
    )
    safety_settings = [ # Adjust as needed
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]
    gemini_model = genai.GenerativeModel(
        model_name='gemini-1.5-flash-latest', # Or 'gemini-1.5-pro-latest'
        generation_config=generation_config,
        safety_settings=safety_settings
        )
    print("Gemini model initialized.")
    gemini_enabled = True
except Exception as e:
    print(f"Failed to initialize Gemini model: {e}", file=sys.stderr)
    gemini_enabled = False


def get_chatbot_response(user_message):
    """Determines the response based on user message, potentially using Gemini."""
    print(f"Processing message: {user_message}")

    if not gemini_enabled:
        return "Sorry, the AI enhancement feature is currently unavailable."

    # === Strategy: Use Gemini for Response Generation after fetching data ===
    # (Simpler than NLU first, good starting point)

    # 1. Basic Logic to Determine Intent (Keep your simple logic for now)
    data_source = None
    query_term = user_message # Use the whole message or extract simply
    data_result = None

    if "local keyword" in user_message.lower(): # Replace with your keyword/logic
        data_source = "local"
        data_result = query_local_data(query_term)
    elif "api keyword" in user_message.lower(): # Replace with your keyword/logic
        data_source = "api"
        data_result = query_api(query_term)
    else:
        # Default or ask Gemini for general response?
        pass # Handle later

    # 2. Use Gemini to Generate Response Based on Data (or lack thereof)
    try:
        prompt = ""
        if data_result and "Error" not in data_result and "not found" not in data_result:
            # We found data! Ask Gemini to make it conversational.
            prompt = f"""The user asked: "{user_message}"
Based on the {data_source} data source, we found the following information:
---
{data_result}
---
Please formulate a helpful and conversational response based ONLY on the information provided above. Do not make things up. If the data is tabular, summarize it nicely."""
        elif data_result and ("Error" in data_result or "not found" in data_result):
            # There was an error or nothing was found by our specific functions
            prompt = f"""The user asked: "{user_message}"
I tried looking this up in my {data_source or 'specific'} data source, but encountered an issue or couldn't find a direct match ('{data_result}').
Can you provide a helpful general response, acknowledge the lookup attempt failed for specific data, or suggest how the user could rephrase their query for my specific data sources ([Describe your sources briefly])?"""
        else:
            # The initial keyword logic didn't match. Ask Gemini for general help.
            prompt = f"""The user asked: "{user_message}"
I primarily answer questions about [Your Local Data Topic] using a local file and [Your API Topic] using a live API.
Please provide a helpful general response to the user, or if their query seems related to my topics but missed the keywords, suggest how they might ask more specifically."""

        print(f"Sending prompt to Gemini: {prompt[:100]}...") # Log start of prompt
        response = gemini_model.generate_content(prompt)
        print("Received response from Gemini.")
        # Accessing the text safely:
        response_text = ''.join(part.text for part in response.parts) if response.parts else f"Gemini finished but returned no text content. Finish reason: {response.prompt_feedback.block_reason or 'OK'}"
        # You might also check response.prompt_feedback for safety blocks
        if response.prompt_feedback and response.prompt_feedback.block_reason:
             print(f"Gemini response blocked. Reason: {response.prompt_feedback.block_reason}")
             return f"My response was blocked due to safety settings ({response.prompt_feedback.block_reason}). Please rephrase your request."
        return response_text

    except Exception as e:
        print(f"Error interacting with Gemini: {e}", file=sys.stderr)
        # Fallback to a non-Gemini response if the API call fails
        if data_result and "Error" not in data_result and "not found" not in data_result:
             return f"I found this, but had trouble phrasing it nicely: {data_result}"
        else:
            return "Sorry, I encountered an error trying to process your request with AI enhancement."


# --- Flask Application Section ---
app = Flask(__name__)

@app.route('/chat', methods=['POST'])
def chat_endpoint():
    """Endpoint to handle chat messages."""
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({"error": "Request must be JSON with a 'message' key"}), 400

        user_message = data['message']
        if not isinstance(user_message, str) or not user_message.strip():
             return jsonify({"error": "Message must be a non-empty string"}), 400

        print(f"Received message: {user_message}")
        response_text = get_chatbot_response(user_message)

        return jsonify({"response": response_text})

    except Exception as e:
        # Log the exception for debugging
        print(f"Error in /chat endpoint: {e}", file=sys.stderr) # Log to stderr
        # Consider using Flask's logger: app.logger.error(f"Error in /chat endpoint: {e}")
        return jsonify({"error": "An internal server error occurred"}), 500

# Optional: Simple route for testing if the server is running
@app.route('/', methods=['GET'])
def index():
    return "Chatbot server is running."

# --- Main Execution Block ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run Chatbot Flask App or ETL Pipeline.')
    parser.add_argument('--run-etl', action='store_true', help='Run the ETL pipeline instead of the Flask server.')
    # Add other potential arguments here if needed

    args = parser.parse_args()

    if args.run_etl:
        print("Running ETL Pipeline...")
        try:
            run_etl_pipeline()
            print("ETL Pipeline finished.")
        except Exception as e:
            print(f"ETL Pipeline failed: {e}", file=sys.stderr)
            sys.exit(1) # Exit with error code if ETL fails
    else:
        print("Starting Flask Chatbot Server...")
        # Use host='0.0.0.0' to make it accessible on your network
        # Use a port like 5000 or one suitable for your setup
        # Debug=True is helpful for development but should be False in production
        app.run(host='0.0.0.0', port=5000, debug=True)